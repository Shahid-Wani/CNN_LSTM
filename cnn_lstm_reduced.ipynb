{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "Found 8 CSV files: ['Friday-WorkingHours-Afternoon-DDos.csv', 'Friday-WorkingHours-Afternoon-PortScan.csv', 'Friday-WorkingHours-Morning.csv', 'Monday-WorkingHours.csv', 'Thursday-WorkingHours-Afternoon-Infilteration.csv', 'Thursday-WorkingHours-Morning-WebAttacks.csv', 'Tuesday-WorkingHours.csv', 'Wednesday-workingHours.csv']\n",
      "\n",
      "Loading Friday-WorkingHours-Afternoon-DDos.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Shape: (225745, 79)\n",
      "\n",
      "Loading Friday-WorkingHours-Afternoon-PortScan.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Shape: (286467, 79)\n",
      "\n",
      "Loading Friday-WorkingHours-Morning.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Shape: (191033, 79)\n",
      "\n",
      "Loading Monday-WorkingHours.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Shape: (529918, 79)\n",
      "\n",
      "Loading Thursday-WorkingHours-Afternoon-Infilteration.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Shape: (288602, 79)\n",
      "\n",
      "Loading Thursday-WorkingHours-Morning-WebAttacks.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Shape: (170366, 79)\n",
      "\n",
      "Loading Tuesday-WorkingHours.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Shape: (445909, 79)\n",
      "\n",
      "Loading Wednesday-workingHours.csv...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Shape: (692703, 79)\n",
      "\n",
      "Combining all datasets...\n",
      "Final combined shape: (2830743, 79)\n",
      "\n",
      "Cleaning data...\n",
      "Saved cleaned chunk 0 to disk.\n",
      "Cleaning Progress: 3.53%\n",
      "Saved cleaned chunk 1 to disk.\n",
      "Cleaning Progress: 7.07%\n",
      "Saved cleaned chunk 2 to disk.\n",
      "Cleaning Progress: 10.60%\n",
      "Saved cleaned chunk 3 to disk.\n",
      "Cleaning Progress: 14.13%\n",
      "Saved cleaned chunk 4 to disk.\n",
      "Cleaning Progress: 17.66%\n",
      "Saved cleaned chunk 5 to disk.\n",
      "Cleaning Progress: 21.20%\n",
      "Saved cleaned chunk 6 to disk.\n",
      "Cleaning Progress: 24.73%\n",
      "Saved cleaned chunk 7 to disk.\n",
      "Cleaning Progress: 28.26%\n",
      "Saved cleaned chunk 8 to disk.\n",
      "Cleaning Progress: 31.79%\n",
      "Saved cleaned chunk 9 to disk.\n",
      "Cleaning Progress: 35.33%\n",
      "Saved cleaned chunk 10 to disk.\n",
      "Cleaning Progress: 38.86%\n",
      "Saved cleaned chunk 11 to disk.\n",
      "Cleaning Progress: 42.39%\n",
      "Saved cleaned chunk 12 to disk.\n",
      "Cleaning Progress: 45.92%\n",
      "Saved cleaned chunk 13 to disk.\n",
      "Cleaning Progress: 49.46%\n",
      "Saved cleaned chunk 14 to disk.\n",
      "Cleaning Progress: 52.99%\n",
      "Saved cleaned chunk 15 to disk.\n",
      "Cleaning Progress: 56.52%\n",
      "Saved cleaned chunk 16 to disk.\n",
      "Cleaning Progress: 60.05%\n",
      "Saved cleaned chunk 17 to disk.\n",
      "Cleaning Progress: 63.59%\n",
      "Saved cleaned chunk 18 to disk.\n",
      "Cleaning Progress: 67.12%\n",
      "Saved cleaned chunk 19 to disk.\n",
      "Cleaning Progress: 70.65%\n",
      "Saved cleaned chunk 20 to disk.\n",
      "Cleaning Progress: 74.19%\n",
      "Saved cleaned chunk 21 to disk.\n",
      "Cleaning Progress: 77.72%\n",
      "Saved cleaned chunk 22 to disk.\n",
      "Cleaning Progress: 81.25%\n",
      "Saved cleaned chunk 23 to disk.\n",
      "Cleaning Progress: 84.78%\n",
      "Saved cleaned chunk 24 to disk.\n",
      "Cleaning Progress: 88.32%\n",
      "Saved cleaned chunk 25 to disk.\n",
      "Cleaning Progress: 91.85%\n",
      "Saved cleaned chunk 26 to disk.\n",
      "Cleaning Progress: 95.38%\n",
      "Saved cleaned chunk 27 to disk.\n",
      "Cleaning Progress: 98.91%\n",
      "Saved cleaned chunk 28 to disk.\n",
      "Cleaning Progress: 100.00%\n",
      "\n",
      "Available columns in dataset:\n",
      "[' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', ' Label']\n",
      "\n",
      "Using selected features:\n",
      "[' Flow IAT Min', ' Bwd Avg Packets/Bulk', 'Fwd PSH Flags', 'Init_Win_bytes_forward', ' PSH Flag Count', ' Bwd URG Flags', ' Fwd Packet Length Mean', ' Fwd IAT Std', 'Fwd Packets/s', 'Bwd Avg Bulk Rate', ' Destination Port', 'Idle Mean', ' Packet Length Mean', ' Flow Duration', 'Active Mean', ' Active Min', ' Total Length of Bwd Packets', ' Bwd Header Length', ' Subflow Fwd Bytes', ' Total Fwd Packets']\n",
      "\n",
      "Preparing features and labels...\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Balancing dataset...\n",
      "\n",
      "Balancing dataset...\n",
      "\n",
      "Original class distribution:\n",
      "Class 0: 2273097 samples\n",
      "Class 1: 1966 samples\n",
      "Class 2: 128027 samples\n",
      "Class 3: 10293 samples\n",
      "Class 4: 231073 samples\n",
      "Class 5: 5499 samples\n",
      "Class 6: 5796 samples\n",
      "Class 7: 7938 samples\n",
      "Class 8: 11 samples\n",
      "Class 9: 36 samples\n",
      "Class 10: 158930 samples\n",
      "Class 11: 5897 samples\n",
      "Class 12: 1507 samples\n",
      "Class 13: 21 samples\n",
      "Class 14: 652 samples\n",
      "\n",
      "Downsampling classes with more than max_samples_per_class...\n",
      "Class 0 downsampled to 10000 samples.\n",
      "Class 1 retained with 1966 samples.\n",
      "Class 2 downsampled to 10000 samples.\n",
      "Class 3 downsampled to 10000 samples.\n",
      "Class 4 downsampled to 10000 samples.\n",
      "Class 5 retained with 5499 samples.\n",
      "Class 6 retained with 5796 samples.\n",
      "Class 7 retained with 7938 samples.\n",
      "Class 8 retained with 11 samples.\n",
      "Class 9 retained with 36 samples.\n",
      "Class 10 downsampled to 10000 samples.\n",
      "Class 11 retained with 5897 samples.\n",
      "Class 12 retained with 1507 samples.\n",
      "Class 13 retained with 21 samples.\n",
      "Class 14 retained with 652 samples.\n",
      "\n",
      "After downsampling:\n",
      "Class 0: 10000 samples\n",
      "Class 1: 1966 samples\n",
      "Class 2: 10000 samples\n",
      "Class 3: 10000 samples\n",
      "Class 4: 10000 samples\n",
      "Class 5: 5499 samples\n",
      "Class 6: 5796 samples\n",
      "Class 7: 7938 samples\n",
      "Class 8: 11 samples\n",
      "Class 9: 36 samples\n",
      "Class 10: 10000 samples\n",
      "Class 11: 5897 samples\n",
      "Class 12: 1507 samples\n",
      "Class 13: 21 samples\n",
      "Class 14: 652 samples\n",
      "\n",
      "Applying Standard SMOTE to classes [8, 9, 13] to increase to 10000 samples...\n",
      "\n",
      "After applying Standard SMOTE:\n",
      "Class 0: 10000 samples\n",
      "Class 1: 1966 samples\n",
      "Class 2: 10000 samples\n",
      "Class 3: 10000 samples\n",
      "Class 4: 10000 samples\n",
      "Class 5: 5499 samples\n",
      "Class 6: 5796 samples\n",
      "Class 7: 7938 samples\n",
      "Class 8: 10000 samples\n",
      "Class 9: 10000 samples\n",
      "Class 10: 10000 samples\n",
      "Class 11: 5897 samples\n",
      "Class 12: 1507 samples\n",
      "Class 13: 10000 samples\n",
      "Class 14: 652 samples\n",
      "\n",
      "Oversampling classes [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 14] using Cluster-Based SMOTE to 10000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final class distribution:\n",
      "Class 0: 10000 samples\n",
      "Class 1: 10003 samples\n",
      "Class 2: 10000 samples\n",
      "Class 3: 10000 samples\n",
      "Class 4: 10000 samples\n",
      "Class 5: 10006 samples\n",
      "Class 6: 10004 samples\n",
      "Class 7: 10001 samples\n",
      "Class 8: 10000 samples\n",
      "Class 9: 10000 samples\n",
      "Class 10: 10000 samples\n",
      "Class 11: 10003 samples\n",
      "Class 12: 10000 samples\n",
      "Class 13: 10000 samples\n",
      "Class 14: 10000 samples\n",
      "\n",
      "Creating sequences...\n",
      "\n",
      "Creating sequences for LSTM...\n",
      "Original input shape: (150017, 20)\n",
      "Final sequence shape: (150008, 10, 20)\n",
      "\n",
      "Splitting data...\n",
      "\n",
      "Validating data shapes:\n",
      "X_train shape: (120006, 10, 20)\n",
      "y_train shape: (120006,)\n",
      "X_test shape: (30002, 10, 20)\n",
      "y_test shape: (30002,)\n",
      "\n",
      "Creating and training improved model...\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738583542.528629    1595 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">975</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m3,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m975\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,191</span> (539.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,191\u001b[0m (539.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">137,423</span> (536.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m137,423\u001b[0m (536.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8834 - loss: 1.8578\n",
      "Epoch 1: val_loss improved from inf to 0.28417, saving model to checkpoints/model_epoch_01.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 10ms/step - accuracy: 0.8834 - loss: 1.8572 - val_accuracy: 0.9919 - val_loss: 0.2842 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9741 - loss: 0.3428\n",
      "Epoch 2: val_loss improved from 0.28417 to 0.27023, saving model to checkpoints/model_epoch_02.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9741 - loss: 0.3428 - val_accuracy: 0.9939 - val_loss: 0.2702 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9757 - loss: 0.3276\n",
      "Epoch 3: val_loss improved from 0.27023 to 0.24819, saving model to checkpoints/model_epoch_03.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9757 - loss: 0.3276 - val_accuracy: 0.9940 - val_loss: 0.2482 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9759 - loss: 0.3094\n",
      "Epoch 4: val_loss improved from 0.24819 to 0.24023, saving model to checkpoints/model_epoch_04.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9759 - loss: 0.3094 - val_accuracy: 0.9905 - val_loss: 0.2402 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9764 - loss: 0.2954\n",
      "Epoch 5: val_loss improved from 0.24023 to 0.22304, saving model to checkpoints/model_epoch_05.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9764 - loss: 0.2954 - val_accuracy: 0.9952 - val_loss: 0.2230 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.2860\n",
      "Epoch 6: val_loss improved from 0.22304 to 0.22129, saving model to checkpoints/model_epoch_06.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.2860 - val_accuracy: 0.9933 - val_loss: 0.2213 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9777 - loss: 0.2776\n",
      "Epoch 7: val_loss improved from 0.22129 to 0.20050, saving model to checkpoints/model_epoch_07.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9777 - loss: 0.2776 - val_accuracy: 0.9938 - val_loss: 0.2005 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9771 - loss: 0.2665\n",
      "Epoch 8: val_loss improved from 0.20050 to 0.16813, saving model to checkpoints/model_epoch_08.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9771 - loss: 0.2665 - val_accuracy: 0.9952 - val_loss: 0.1681 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9778 - loss: 0.2651\n",
      "Epoch 9: val_loss did not improve from 0.16813\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9778 - loss: 0.2651 - val_accuracy: 0.9940 - val_loss: 0.1736 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9769 - loss: 0.2615\n",
      "Epoch 10: val_loss did not improve from 0.16813\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9769 - loss: 0.2615 - val_accuracy: 0.9943 - val_loss: 0.2008 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.2623\n",
      "Epoch 11: val_loss did not improve from 0.16813\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.2623 - val_accuracy: 0.9934 - val_loss: 0.1785 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.2530\n",
      "Epoch 12: val_loss did not improve from 0.16813\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9776 - loss: 0.2530 - val_accuracy: 0.9942 - val_loss: 0.1942 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9791 - loss: 0.2436\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16813\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9791 - loss: 0.2436 - val_accuracy: 0.9925 - val_loss: 0.1903 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1690\n",
      "Epoch 14: val_loss improved from 0.16813 to 0.10859, saving model to checkpoints/model_epoch_14.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1690 - val_accuracy: 0.9964 - val_loss: 0.1086 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9855 - loss: 0.1489\n",
      "Epoch 15: val_loss did not improve from 0.10859\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9855 - loss: 0.1489 - val_accuracy: 0.9946 - val_loss: 0.1183 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1417\n",
      "Epoch 16: val_loss did not improve from 0.10859\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1417 - val_accuracy: 0.9954 - val_loss: 0.1117 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1427\n",
      "Epoch 17: val_loss did not improve from 0.10859\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1427 - val_accuracy: 0.9964 - val_loss: 0.1089 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.1431\n",
      "Epoch 18: val_loss improved from 0.10859 to 0.10388, saving model to checkpoints/model_epoch_18.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.1431 - val_accuracy: 0.9963 - val_loss: 0.1039 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.1394\n",
      "Epoch 19: val_loss improved from 0.10388 to 0.10052, saving model to checkpoints/model_epoch_19.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.1394 - val_accuracy: 0.9966 - val_loss: 0.1005 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.1421\n",
      "Epoch 20: val_loss did not improve from 0.10052\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.1421 - val_accuracy: 0.9944 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.1435\n",
      "Epoch 21: val_loss did not improve from 0.10052\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.1435 - val_accuracy: 0.9965 - val_loss: 0.1036 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.1423\n",
      "Epoch 22: val_loss did not improve from 0.10052\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.1423 - val_accuracy: 0.9964 - val_loss: 0.1060 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1415\n",
      "Epoch 23: val_loss improved from 0.10052 to 0.09775, saving model to checkpoints/model_epoch_23.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1415 - val_accuracy: 0.9968 - val_loss: 0.0977 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.1436\n",
      "Epoch 24: val_loss did not improve from 0.09775\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.1436 - val_accuracy: 0.9959 - val_loss: 0.1055 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.1440\n",
      "Epoch 25: val_loss did not improve from 0.09775\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.1440 - val_accuracy: 0.9964 - val_loss: 0.1033 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1401\n",
      "Epoch 26: val_loss did not improve from 0.09775\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.1401 - val_accuracy: 0.9960 - val_loss: 0.1030 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9868 - loss: 0.1402\n",
      "Epoch 27: val_loss did not improve from 0.09775\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9868 - loss: 0.1402 - val_accuracy: 0.9967 - val_loss: 0.0995 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.1436\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09775\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.1436 - val_accuracy: 0.9959 - val_loss: 0.1001 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.1042\n",
      "Epoch 29: val_loss improved from 0.09775 to 0.06441, saving model to checkpoints/model_epoch_29.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.1042 - val_accuracy: 0.9966 - val_loss: 0.0644 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9908 - loss: 0.0889\n",
      "Epoch 30: val_loss did not improve from 0.06441\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9908 - loss: 0.0889 - val_accuracy: 0.9962 - val_loss: 0.0645 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9912 - loss: 0.0853\n",
      "Epoch 31: val_loss did not improve from 0.06441\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9912 - loss: 0.0853 - val_accuracy: 0.9968 - val_loss: 0.0646 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9913 - loss: 0.0849\n",
      "Epoch 32: val_loss improved from 0.06441 to 0.06323, saving model to checkpoints/model_epoch_32.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9913 - loss: 0.0849 - val_accuracy: 0.9965 - val_loss: 0.0632 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9914 - loss: 0.0826\n",
      "Epoch 33: val_loss improved from 0.06323 to 0.06223, saving model to checkpoints/model_epoch_33.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9914 - loss: 0.0826 - val_accuracy: 0.9966 - val_loss: 0.0622 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9904 - loss: 0.0888\n",
      "Epoch 34: val_loss did not improve from 0.06223\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9904 - loss: 0.0888 - val_accuracy: 0.9968 - val_loss: 0.0638 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9907 - loss: 0.0868\n",
      "Epoch 35: val_loss did not improve from 0.06223\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9907 - loss: 0.0868 - val_accuracy: 0.9968 - val_loss: 0.0643 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9915 - loss: 0.0838\n",
      "Epoch 36: val_loss did not improve from 0.06223\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9915 - loss: 0.0838 - val_accuracy: 0.9963 - val_loss: 0.0666 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9914 - loss: 0.0836\n",
      "Epoch 37: val_loss did not improve from 0.06223\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9914 - loss: 0.0836 - val_accuracy: 0.9963 - val_loss: 0.0660 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.0852\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06223\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.0852 - val_accuracy: 0.9951 - val_loss: 0.0686 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9938 - loss: 0.0661\n",
      "Epoch 39: val_loss improved from 0.06223 to 0.04632, saving model to checkpoints/model_epoch_39.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9938 - loss: 0.0661 - val_accuracy: 0.9969 - val_loss: 0.0463 - learning_rate: 1.2500e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0564\n",
      "Epoch 40: val_loss improved from 0.04632 to 0.04420, saving model to checkpoints/model_epoch_40.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0564 - val_accuracy: 0.9967 - val_loss: 0.0442 - learning_rate: 1.2500e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9931 - loss: 0.0581\n",
      "Epoch 41: val_loss improved from 0.04420 to 0.04165, saving model to checkpoints/model_epoch_41.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9931 - loss: 0.0581 - val_accuracy: 0.9969 - val_loss: 0.0416 - learning_rate: 1.2500e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0559\n",
      "Epoch 42: val_loss improved from 0.04165 to 0.04148, saving model to checkpoints/model_epoch_42.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0559 - val_accuracy: 0.9969 - val_loss: 0.0415 - learning_rate: 1.2500e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9937 - loss: 0.0550\n",
      "Epoch 43: val_loss did not improve from 0.04148\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9937 - loss: 0.0550 - val_accuracy: 0.9971 - val_loss: 0.0417 - learning_rate: 1.2500e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0550\n",
      "Epoch 44: val_loss did not improve from 0.04148\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0550 - val_accuracy: 0.9971 - val_loss: 0.0423 - learning_rate: 1.2500e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0549\n",
      "Epoch 45: val_loss improved from 0.04148 to 0.04111, saving model to checkpoints/model_epoch_45.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0549 - val_accuracy: 0.9970 - val_loss: 0.0411 - learning_rate: 1.2500e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9934 - loss: 0.0542\n",
      "Epoch 46: val_loss improved from 0.04111 to 0.03971, saving model to checkpoints/model_epoch_46.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9934 - loss: 0.0542 - val_accuracy: 0.9970 - val_loss: 0.0397 - learning_rate: 1.2500e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0558\n",
      "Epoch 47: val_loss did not improve from 0.03971\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0558 - val_accuracy: 0.9971 - val_loss: 0.0414 - learning_rate: 1.2500e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - loss: 0.0534\n",
      "Epoch 48: val_loss did not improve from 0.03971\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9940 - loss: 0.0534 - val_accuracy: 0.9951 - val_loss: 0.0465 - learning_rate: 1.2500e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0555\n",
      "Epoch 49: val_loss did not improve from 0.03971\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0555 - val_accuracy: 0.9970 - val_loss: 0.0418 - learning_rate: 1.2500e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0526\n",
      "Epoch 50: val_loss did not improve from 0.03971\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0526 - val_accuracy: 0.9970 - val_loss: 0.0405 - learning_rate: 1.2500e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0558\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.03971\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9933 - loss: 0.0558 - val_accuracy: 0.9969 - val_loss: 0.0416 - learning_rate: 1.2500e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0465\n",
      "Epoch 52: val_loss improved from 0.03971 to 0.03371, saving model to checkpoints/model_epoch_52.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0465 - val_accuracy: 0.9971 - val_loss: 0.0337 - learning_rate: 6.2500e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0421\n",
      "Epoch 53: val_loss improved from 0.03371 to 0.03167, saving model to checkpoints/model_epoch_53.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0421 - val_accuracy: 0.9971 - val_loss: 0.0317 - learning_rate: 6.2500e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0406\n",
      "Epoch 54: val_loss improved from 0.03167 to 0.03101, saving model to checkpoints/model_epoch_54.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0406 - val_accuracy: 0.9972 - val_loss: 0.0310 - learning_rate: 6.2500e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9948 - loss: 0.0404\n",
      "Epoch 55: val_loss did not improve from 0.03101\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9948 - loss: 0.0404 - val_accuracy: 0.9971 - val_loss: 0.0313 - learning_rate: 6.2500e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9952 - loss: 0.0392\n",
      "Epoch 56: val_loss improved from 0.03101 to 0.02942, saving model to checkpoints/model_epoch_56.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9952 - loss: 0.0392 - val_accuracy: 0.9972 - val_loss: 0.0294 - learning_rate: 6.2500e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0382\n",
      "Epoch 57: val_loss did not improve from 0.02942\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0382 - val_accuracy: 0.9972 - val_loss: 0.0298 - learning_rate: 6.2500e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0373\n",
      "Epoch 58: val_loss improved from 0.02942 to 0.02895, saving model to checkpoints/model_epoch_58.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0373 - val_accuracy: 0.9972 - val_loss: 0.0290 - learning_rate: 6.2500e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9953 - loss: 0.0381\n",
      "Epoch 59: val_loss did not improve from 0.02895\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9953 - loss: 0.0381 - val_accuracy: 0.9972 - val_loss: 0.0290 - learning_rate: 6.2500e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0385\n",
      "Epoch 60: val_loss improved from 0.02895 to 0.02888, saving model to checkpoints/model_epoch_60.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0385 - val_accuracy: 0.9972 - val_loss: 0.0289 - learning_rate: 6.2500e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0379\n",
      "Epoch 61: val_loss improved from 0.02888 to 0.02861, saving model to checkpoints/model_epoch_61.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0379 - val_accuracy: 0.9972 - val_loss: 0.0286 - learning_rate: 6.2500e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9946 - loss: 0.0389\n",
      "Epoch 62: val_loss did not improve from 0.02861\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9946 - loss: 0.0389 - val_accuracy: 0.9971 - val_loss: 0.0287 - learning_rate: 6.2500e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0386\n",
      "Epoch 63: val_loss did not improve from 0.02861\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0386 - val_accuracy: 0.9970 - val_loss: 0.0290 - learning_rate: 6.2500e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0384\n",
      "Epoch 64: val_loss improved from 0.02861 to 0.02853, saving model to checkpoints/model_epoch_64.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0383 - val_accuracy: 0.9972 - val_loss: 0.0285 - learning_rate: 6.2500e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0376\n",
      "Epoch 65: val_loss did not improve from 0.02853\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9951 - loss: 0.0376 - val_accuracy: 0.9972 - val_loss: 0.0288 - learning_rate: 6.2500e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9950 - loss: 0.0374\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.02853\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9950 - loss: 0.0374 - val_accuracy: 0.9969 - val_loss: 0.0294 - learning_rate: 6.2500e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0340\n",
      "Epoch 67: val_loss improved from 0.02853 to 0.02592, saving model to checkpoints/model_epoch_67.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0340 - val_accuracy: 0.9973 - val_loss: 0.0259 - learning_rate: 3.1250e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0316\n",
      "Epoch 68: val_loss improved from 0.02592 to 0.02408, saving model to checkpoints/model_epoch_68.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0316 - val_accuracy: 0.9973 - val_loss: 0.0241 - learning_rate: 3.1250e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0316\n",
      "Epoch 69: val_loss improved from 0.02408 to 0.02398, saving model to checkpoints/model_epoch_69.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0316 - val_accuracy: 0.9973 - val_loss: 0.0240 - learning_rate: 3.1250e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0307\n",
      "Epoch 70: val_loss did not improve from 0.02398\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0307 - val_accuracy: 0.9972 - val_loss: 0.0241 - learning_rate: 3.1250e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0298\n",
      "Epoch 71: val_loss improved from 0.02398 to 0.02286, saving model to checkpoints/model_epoch_71.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0298 - val_accuracy: 0.9973 - val_loss: 0.0229 - learning_rate: 3.1250e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0299\n",
      "Epoch 72: val_loss did not improve from 0.02286\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0299 - val_accuracy: 0.9972 - val_loss: 0.0237 - learning_rate: 3.1250e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0285\n",
      "Epoch 73: val_loss improved from 0.02286 to 0.02248, saving model to checkpoints/model_epoch_73.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0285 - val_accuracy: 0.9973 - val_loss: 0.0225 - learning_rate: 3.1250e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0295\n",
      "Epoch 74: val_loss did not improve from 0.02248\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0295 - val_accuracy: 0.9973 - val_loss: 0.0234 - learning_rate: 3.1250e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0293\n",
      "Epoch 75: val_loss improved from 0.02248 to 0.02193, saving model to checkpoints/model_epoch_75.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0293 - val_accuracy: 0.9973 - val_loss: 0.0219 - learning_rate: 3.1250e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0286\n",
      "Epoch 76: val_loss did not improve from 0.02193\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0286 - val_accuracy: 0.9973 - val_loss: 0.0221 - learning_rate: 3.1250e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0277\n",
      "Epoch 77: val_loss did not improve from 0.02193\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0277 - val_accuracy: 0.9972 - val_loss: 0.0220 - learning_rate: 3.1250e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0283\n",
      "Epoch 78: val_loss improved from 0.02193 to 0.02157, saving model to checkpoints/model_epoch_78.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0283 - val_accuracy: 0.9976 - val_loss: 0.0216 - learning_rate: 3.1250e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0285\n",
      "Epoch 79: val_loss did not improve from 0.02157\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0285 - val_accuracy: 0.9972 - val_loss: 0.0219 - learning_rate: 3.1250e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0278\n",
      "Epoch 80: val_loss did not improve from 0.02157\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0278 - val_accuracy: 0.9972 - val_loss: 0.0230 - learning_rate: 3.1250e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0285\n",
      "Epoch 81: val_loss did not improve from 0.02157\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0285 - val_accuracy: 0.9973 - val_loss: 0.0217 - learning_rate: 3.1250e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0283\n",
      "Epoch 82: val_loss did not improve from 0.02157\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0283 - val_accuracy: 0.9973 - val_loss: 0.0219 - learning_rate: 3.1250e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0285\n",
      "Epoch 83: val_loss improved from 0.02157 to 0.02143, saving model to checkpoints/model_epoch_83.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0285 - val_accuracy: 0.9972 - val_loss: 0.0214 - learning_rate: 3.1250e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0283\n",
      "Epoch 84: val_loss did not improve from 0.02143\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0283 - val_accuracy: 0.9973 - val_loss: 0.0215 - learning_rate: 3.1250e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0287\n",
      "Epoch 85: val_loss improved from 0.02143 to 0.02118, saving model to checkpoints/model_epoch_85.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9959 - loss: 0.0287 - val_accuracy: 0.9975 - val_loss: 0.0212 - learning_rate: 3.1250e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0277\n",
      "Epoch 86: val_loss did not improve from 0.02118\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0277 - val_accuracy: 0.9973 - val_loss: 0.0220 - learning_rate: 3.1250e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0287\n",
      "Epoch 87: val_loss did not improve from 0.02118\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0287 - val_accuracy: 0.9972 - val_loss: 0.0221 - learning_rate: 3.1250e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0278\n",
      "Epoch 88: val_loss did not improve from 0.02118\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0278 - val_accuracy: 0.9973 - val_loss: 0.0212 - learning_rate: 3.1250e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0290\n",
      "Epoch 89: val_loss improved from 0.02118 to 0.02112, saving model to checkpoints/model_epoch_89.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9957 - loss: 0.0290 - val_accuracy: 0.9976 - val_loss: 0.0211 - learning_rate: 3.1250e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0292\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.02112\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0292 - val_accuracy: 0.9973 - val_loss: 0.0223 - learning_rate: 3.1250e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0260\n",
      "Epoch 91: val_loss improved from 0.02112 to 0.02015, saving model to checkpoints/model_epoch_91.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0260 - val_accuracy: 0.9974 - val_loss: 0.0201 - learning_rate: 1.5625e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9967 - loss: 0.0246\n",
      "Epoch 92: val_loss improved from 0.02015 to 0.01996, saving model to checkpoints/model_epoch_92.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9967 - loss: 0.0246 - val_accuracy: 0.9974 - val_loss: 0.0200 - learning_rate: 1.5625e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m5997/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0257\n",
      "Epoch 93: val_loss improved from 0.01996 to 0.01977, saving model to checkpoints/model_epoch_93.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.0257 - val_accuracy: 0.9974 - val_loss: 0.0198 - learning_rate: 1.5625e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0234\n",
      "Epoch 94: val_loss improved from 0.01977 to 0.01900, saving model to checkpoints/model_epoch_94.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0234 - val_accuracy: 0.9976 - val_loss: 0.0190 - learning_rate: 1.5625e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m5999/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0242\n",
      "Epoch 95: val_loss improved from 0.01900 to 0.01887, saving model to checkpoints/model_epoch_95.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0242 - val_accuracy: 0.9975 - val_loss: 0.0189 - learning_rate: 1.5625e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m6000/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0237\n",
      "Epoch 96: val_loss did not improve from 0.01887\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0237 - val_accuracy: 0.9973 - val_loss: 0.0195 - learning_rate: 1.5625e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9967 - loss: 0.0239\n",
      "Epoch 97: val_loss improved from 0.01887 to 0.01878, saving model to checkpoints/model_epoch_97.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9967 - loss: 0.0239 - val_accuracy: 0.9976 - val_loss: 0.0188 - learning_rate: 1.5625e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0234\n",
      "Epoch 98: val_loss improved from 0.01878 to 0.01832, saving model to checkpoints/model_epoch_98.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9966 - loss: 0.0234 - val_accuracy: 0.9977 - val_loss: 0.0183 - learning_rate: 1.5625e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m5996/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - loss: 0.0229\n",
      "Epoch 99: val_loss did not improve from 0.01832\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9968 - loss: 0.0229 - val_accuracy: 0.9975 - val_loss: 0.0185 - learning_rate: 1.5625e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m5998/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - loss: 0.0226\n",
      "Epoch 100: val_loss improved from 0.01832 to 0.01832, saving model to checkpoints/model_epoch_100.weights.h5\n",
      "\u001b[1m6001/6001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.9968 - loss: 0.0226 - val_accuracy: 0.9976 - val_loss: 0.0183 - learning_rate: 1.5625e-05\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All model artifacts saved successfully in model_artifacts\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.9976\n",
      "Precision: 0.9976\n",
      "Recall: 0.9976\n",
      "F1 Score: 0.9976\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    BENIGN       1.00      1.00      1.00      1998\n",
      "                       Bot       1.00      1.00      1.00      2001\n",
      "                      DDoS       1.00      1.00      1.00      2000\n",
      "             DoS GoldenEye       1.00      1.00      1.00      2000\n",
      "                  DoS Hulk       1.00      1.00      1.00      2000\n",
      "          DoS Slowhttptest       1.00      1.00      1.00      2001\n",
      "             DoS slowloris       1.00      1.00      1.00      2001\n",
      "               FTP-Patator       1.00      1.00      1.00      2000\n",
      "                Heartbleed       1.00      1.00      1.00      2000\n",
      "              Infiltration       1.00      1.00      1.00      2000\n",
      "                  PortScan       1.00      1.00      1.00      2000\n",
      "               SSH-Patator       1.00      1.00      1.00      2001\n",
      "  Web Attack � Brute Force       0.98      1.00      0.99      2000\n",
      "Web Attack � Sql Injection       1.00      1.00      1.00      2000\n",
      "          Web Attack � XSS       1.00      0.98      0.99      2000\n",
      "\n",
      "                  accuracy                           1.00     30002\n",
      "                 macro avg       1.00      1.00      1.00     30002\n",
      "              weighted avg       1.00      1.00      1.00     30002\n",
      "\n",
      "\n",
      "No training history available to plot\n",
      "\n",
      "Generating distribution visualizations...\n",
      "\n",
      "Generating PCA visualization...\n",
      "\n",
      "Generating t-SNE visualization...\n",
      "\n",
      "Visualization artifacts have been saved to 'model_artifacts/visualizations'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import optuna\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import shutil\n",
    "import tensorflow.keras.mixed_precision as mixed_precision\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "VIS_DIR = \"model_artifacts/visualizations\"\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f'model_training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# stream handler, also show logs in console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "os.environ.setdefault('DATASET_PATH', r\"/teamspace/studios/this_studio/CIC-IDS 2017\")\n",
    "\n",
    "input_shape = None\n",
    "n_classes = None\n",
    "\n",
    "# 1. Data Preparation\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_and_clean_data(self, data):\n",
    "        \"\"\"Enhanced data cleaning with better handling of outliers and invalid values\"\"\"\n",
    "        try:\n",
    "            # Convert all columns to numeric, except 'Label'\n",
    "            numeric_columns = [col for col in data.columns if col != ' Label']\n",
    "            \n",
    "            # First, convert all numeric columns to float64\n",
    "            for col in numeric_columns:\n",
    "                data[col] = data[col].astype('float64')\n",
    "            \n",
    "            # Process in smaller chunks for memory efficiency\n",
    "            chunk_size = 100000\n",
    "            total_rows = len(data)\n",
    "            \n",
    "            for i in range(0, total_rows, chunk_size):\n",
    "                end_idx = min(i + chunk_size, total_rows)\n",
    "                chunk = data.iloc[i:end_idx].copy()\n",
    "                \n",
    "                # Calculate statistics for outlier detection\n",
    "                Q1 = chunk[numeric_columns].quantile(0.25)\n",
    "                Q3 = chunk[numeric_columns].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Replace outliers with bounds (now all columns are float64)\n",
    "                for col in numeric_columns:\n",
    "                    chunk.loc[chunk[col] < lower_bound[col], col] = lower_bound[col]\n",
    "                    chunk.loc[chunk[col] > upper_bound[col], col] = upper_bound[col]\n",
    "                \n",
    "                # Replace infinities with NaN\n",
    "                chunk = chunk.replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                # Fill NaN with median\n",
    "                chunk = chunk.fillna(chunk[numeric_columns].median())\n",
    "                \n",
    "                # Save cleaned chunk to CSV\n",
    "                chunk.to_csv(f'cleaned_data_chunk_{i // chunk_size}.csv', index=False)\n",
    "                print(f\"Saved cleaned chunk {i // chunk_size} to disk.\")\n",
    "                \n",
    "                # Update the original dataframe\n",
    "                data.iloc[i:end_idx] = chunk\n",
    "                \n",
    "                progress = (end_idx / total_rows) * 100\n",
    "                print(f\"Cleaning Progress: {progress:.2f}%\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data cleaning: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def prepare_features_and_labels(self, data, feature_columns, label_column):\n",
    "        \"\"\"Prepare features and labels\"\"\"\n",
    "        # Separate features and labels\n",
    "        X = data[feature_columns]\n",
    "        y = data[label_column]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        return X_scaled, y_encoded\n",
    "    \n",
    "    def balance_dataset(self, X, y, max_samples_per_class=10000):\n",
    "        \"\"\"\n",
    "        Balance the dataset:\n",
    "        - Downsample classes with > `max_samples_per_class` to `max_samples_per_class`.\n",
    "        - Apply Standard SMOTE to classes with < 50 samples to increase to `max_samples_per_class`.\n",
    "        - Apply Cluster-Based SMOTE to classes with ≥ 50 samples to increase to `max_samples_per_class`.\n",
    "        \"\"\"\n",
    "        print(\"\\nBalancing dataset...\")\n",
    "        try:\n",
    "            # Step 1: Log original class distribution\n",
    "            unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "            print(\"\\nOriginal class distribution:\")\n",
    "            for cls, count in zip(unique_classes, class_counts):\n",
    "                print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "            # Step 2: Downsample classes with > max_samples_per_class\n",
    "            print(\"\\nDownsampling classes with more than max_samples_per_class...\")\n",
    "            X_balanced_list = []\n",
    "            y_balanced_list = []\n",
    "\n",
    "            for cls in unique_classes:\n",
    "                idx = np.where(y == cls)[0]\n",
    "\n",
    "                if len(idx) > max_samples_per_class:\n",
    "                    # Downsample majority class\n",
    "                    idx = np.random.choice(idx, max_samples_per_class, replace=False)\n",
    "                    print(f\"Class {cls} downsampled to {max_samples_per_class} samples.\")\n",
    "                else:\n",
    "                    print(f\"Class {cls} retained with {len(idx)} samples.\")\n",
    "\n",
    "                # Add to balanced list\n",
    "                X_balanced_list.append(X[idx])\n",
    "                y_balanced_list.append(y[idx])\n",
    "\n",
    "            # Concatenate after downsampling\n",
    "            X_balanced = np.vstack(X_balanced_list)\n",
    "            y_balanced = np.concatenate(y_balanced_list)\n",
    "\n",
    "            print(\"\\nAfter downsampling:\")\n",
    "            unique_classes_ds, class_counts_ds = np.unique(y_balanced, return_counts=True)\n",
    "            for cls, count in zip(unique_classes_ds, class_counts_ds):\n",
    "                print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "            # Step 3: Classify small and large classes for SMOTE\n",
    "            small_classes = [cls for cls, count in zip(unique_classes_ds, class_counts_ds) if count < 50]\n",
    "            large_classes = [cls for cls, count in zip(unique_classes_ds, class_counts_ds) if count >= 50]\n",
    "\n",
    "            # Step 4: Apply Standard SMOTE to Small Classes\n",
    "            if small_classes:\n",
    "                print(\n",
    "                    f\"\\nApplying Standard SMOTE to classes {small_classes} to increase to {max_samples_per_class} samples...\")\n",
    "                smote = SMOTE(sampling_strategy={cls: max_samples_per_class for cls in small_classes}, random_state=42)\n",
    "                X_balanced, y_balanced = smote.fit_resample(X_balanced, y_balanced)\n",
    "\n",
    "                print(\"\\nAfter applying Standard SMOTE:\")\n",
    "                unique_classes_smote, class_counts_smote = np.unique(y_balanced, return_counts=True)\n",
    "                for cls, count in zip(unique_classes_smote, class_counts_smote):\n",
    "                    print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "            # Step 5: Apply Cluster-Based SMOTE to Larger Classes\n",
    "            if large_classes:\n",
    "                print(\n",
    "                    f\"\\nOversampling classes {large_classes} using Cluster-Based SMOTE to {max_samples_per_class} samples...\")\n",
    "                sampling_strategy = {cls: max_samples_per_class for cls in large_classes}\n",
    "\n",
    "                kmeans_smote = KMeansSMOTE(\n",
    "                    sampling_strategy=sampling_strategy,\n",
    "                    random_state=42,\n",
    "                    k_neighbors=5,  # Use 5 neighbors for SMOTE\n",
    "                    cluster_balance_threshold=0.01,  # Balance threshold\n",
    "                    kmeans_estimator=15  # Use 15 clusters for KMeans\n",
    "            )\n",
    "                X_res, y_res = kmeans_smote.fit_resample(X_balanced, y_balanced)\n",
    "            else:\n",
    "                print(\"\\nNo classes require oversampling with Cluster-Based SMOTE.\")\n",
    "                X_res, y_res = X_balanced, y_balanced\n",
    "\n",
    "            print(\"\\nFinal class distribution:\")\n",
    "            unique_final, counts_final = np.unique(y_res, return_counts=True)\n",
    "            for cls, count in zip(unique_final, counts_final):\n",
    "                print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "            return X_res, y_res\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error balancing dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def create_sequences(self, X, y, time_steps=10):\n",
    "        \"\"\"Create sequences with improved feature handling\"\"\"\n",
    "        try:\n",
    "            print(\"\\nCreating sequences for LSTM...\")\n",
    "            print(f\"Original input shape: {X.shape}\")\n",
    "            \n",
    "            # Ensure number of features is appropriate for sequence length\n",
    "            n_features = X.shape[1]\n",
    "            n_features_per_timestep = n_features\n",
    "            \n",
    "            # Reshape maintaining all features\n",
    "            n_samples = X.shape[0] - time_steps + 1\n",
    "            X_seq = np.zeros((n_samples, time_steps, n_features_per_timestep))\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                X_seq[i] = X[i:i + time_steps]\n",
    "            \n",
    "            # Adjust labels to match sequence length\n",
    "            y_seq = y[time_steps-1:]\n",
    "            \n",
    "            print(f\"Final sequence shape: {X_seq.shape}\")\n",
    "            return X_seq, y_seq\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in sequence creation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# 2. Model Architecture\n",
    "def create_model(input_shape, n_classes):\n",
    "    print(f\"\\nCreating model with input shape: {input_shape}\")\n",
    "    model = Sequential([\n",
    "        # CNN layers with regularization\n",
    "        Conv1D(64, kernel_size=3, padding='same', activation='relu', \n",
    "               kernel_regularizer=l2(0.01), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(),  # Replaced MaxPooling1D\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # LSTM layers with regularization\n",
    "        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Bidirectional(LSTM(32, kernel_regularizer=l2(0.01))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers with regularization\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "#Visualizations\n",
    "def create_visualizations(history, y_true, y_pred, y_pred_proba, class_names):\n",
    "    \"\"\"Create comprehensive evaluation visualizations\"\"\"\n",
    "    try:\n",
    "        os.makedirs(VIS_DIR, exist_ok=True)\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names,\n",
    "                    yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIS_DIR, 'confusion_matrix.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. ROC Curves (One-vs-Rest)\n",
    "        y_test_bin = label_binarize(y_true, classes=np.arange(len(class_names)))\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(len(class_names)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "            \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Multi-class ROC Curves')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIS_DIR, 'roc_curves.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Precision-Recall Curves\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(len(class_names)):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], \n",
    "                                                              y_pred_proba[:, i])\n",
    "            plt.plot(recall[i], precision[i], \n",
    "                     label=f'{class_names[i]}')\n",
    "            \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIS_DIR, 'precision_recall_curves.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 4. Training History\n",
    "        if history is not None:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Accuracy plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['accuracy'], label='Train')\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Loss plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['loss'], label='Train')\n",
    "            plt.plot(history.history['val_loss'], label='Validation')\n",
    "            plt.title('Model Loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(VIS_DIR, 'training_history.png'))\n",
    "            plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating visualizations: {str(e)}\")\n",
    "        print(f\"Error creating visualizations: {str(e)}\")\n",
    "        \n",
    "# 3. Training and Evaluation\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, learning_rate=0.001):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.best_model = None\n",
    "        self.history = None\n",
    "    def train(self, X_train, y_train, batch_size=32, epochs=50, validation_split=0.2):\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, label_encoder):\n",
    "        \"\"\"Enhanced evaluation with detailed metrics\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred_classes, average='weighted'\n",
    "        )\n",
    "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Print detailed classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        class_names = label_encoder.classes_\n",
    "        print(classification_report(y_test, y_pred_classes, target_names=class_names))\n",
    "\n",
    "        class_names = label_encoder.classes_\n",
    "        create_visualizations(self.history, y_test, y_pred_classes, y_pred, class_names)\n",
    "        \n",
    "        # Plot training history if available\n",
    "        if hasattr(self.model, 'history') and self.model.history is not None:\n",
    "            history = self.model.history.history\n",
    "            if history and len(history) > 0:  # Check if history exists and is not empty\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                \n",
    "                # Plot accuracy if available\n",
    "                if 'accuracy' in history or 'acc' in history:\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    if 'accuracy' in history:\n",
    "                        plt.plot(history['accuracy'], label='Train')\n",
    "                        if 'val_accuracy' in history:\n",
    "                            plt.plot(history['val_accuracy'], label='Validation')\n",
    "                    elif 'acc' in history:\n",
    "                        plt.plot(history['acc'], label='Train')\n",
    "                        if 'val_acc' in history:\n",
    "                            plt.plot(history['val_acc'], label='Validation')\n",
    "                    plt.title('Model Accuracy')\n",
    "                    plt.ylabel('Accuracy')\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.legend()\n",
    "                \n",
    "                # Plot loss if available\n",
    "                if 'loss' in history:\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.plot(history['loss'], label='Train')\n",
    "                    if 'val_loss' in history:\n",
    "                        plt.plot(history['val_loss'], label='Validation')\n",
    "                    plt.title('Model Loss')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"\\nNo training history available to plot\")\n",
    "        else:\n",
    "            print(\"\\nNo training history available to plot\")\n",
    "\n",
    "    def train_with_improvements(self, X_train, y_train, batch_size=32, epochs=100, validation_split=0.2):\n",
    "        \"\"\"Enhanced training with improvements\"\"\"\n",
    "        # Data augmentation for time series\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.GaussianNoise(0.1)\n",
    "        ])\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_train_aug = data_augmentation(X_train)\n",
    "        X_train = np.concatenate([X_train, X_train_aug], axis=0)\n",
    "        y_train = np.concatenate([y_train, y_train], axis=0)\n",
    "        \n",
    "        # Get callbacks\n",
    "        callbacks = get_advanced_callbacks()\n",
    "        \n",
    "        # Train with class weights\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load dataset from the specified folder\"\"\"\n",
    "    folder_path = os.getenv('DATASET_PATH')\n",
    "    \n",
    "    try:\n",
    "        # List all CSV files in the folder\n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "        print(f\"Found {len(csv_files)} CSV files: {csv_files}\")\n",
    "        \n",
    "        # Load CSV files one by one and process them\n",
    "        all_data = []\n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"\\nLoading {file}...\")\n",
    "            \n",
    "            # Read CSV in chunks\n",
    "            chunk_size = 50000\n",
    "            chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "            \n",
    "            file_data = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                print(f\"Processing chunk {i+1}...\")\n",
    "                file_data.append(chunk)\n",
    "            \n",
    "            # Combine chunks for this file\n",
    "            df = pd.concat(file_data, ignore_index=True)\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            all_data.append(df)\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        print(\"\\nCombining all datasets...\")\n",
    "        combined_data = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Final combined shape: {combined_data.shape}\")\n",
    "        \n",
    "        return combined_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_chunk(chunk, scaler=None):\n",
    "    \"\"\"Process a single data chunk\"\"\"\n",
    "    try:\n",
    "        # Convert numeric columns\n",
    "        numeric_cols = [col for col in chunk.columns if col != ' Label']\n",
    "        chunk[numeric_cols] = chunk[numeric_cols].astype('float64')\n",
    "        \n",
    "        # Handle outliers\n",
    "        Q1 = chunk[numeric_cols].quantile(0.25)\n",
    "        Q3 = chunk[numeric_cols].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        chunk[numeric_cols] = chunk[numeric_cols].clip(\n",
    "            lower=Q1 - 1.5 * IQR,\n",
    "            upper=Q3 + 1.5 * IQR\n",
    "        )\n",
    "        \n",
    "        # Scale if scaler provided\n",
    "        if scaler is not None:\n",
    "            chunk[numeric_cols] = scaler.transform(chunk[numeric_cols])\n",
    "        \n",
    "        return chunk\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing chunk: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def evaluate(self, X_test, y_test, label_encoder):\n",
    "    \"\"\"Enhanced evaluation with detailed metrics\"\"\"\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate detailed metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred_classes, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    class_names = label_encoder.classes_\n",
    "    print(classification_report(y_test, y_pred_classes, target_names=class_names))\n",
    "\n",
    "    # Create visualizations with the updated function signature\n",
    "    create_visualizations(self.history, y_test, y_pred_classes, y_pred, class_names)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. t-SNE Visualization\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)  # Flatten sequences\n",
    "    features_2d = tsne.fit_transform(X_test_flat)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                     c=y_test, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Feature Space Visualization with t-SNE')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.savefig('tsne_visualization.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_advanced_callbacks():\n",
    "    \"\"\"Get advanced callbacks for better training\"\"\"\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}.weights.h5'),\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger('training_log.csv')\n",
    "    ]\n",
    "\n",
    "def create_improved_model(input_shape, n_classes):\n",
    "    \"\"\"Create an improved model with better training speed\"\"\"\n",
    "    # Use mixed precision for faster training\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # First CNN block - reduced complexity\n",
    "        Conv1D(64, kernel_size=3, padding='same', activation='relu', \n",
    "               kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second CNN block - reduced complexity\n",
    "        Conv1D(128, kernel_size=3, padding='same', activation='relu',\n",
    "               kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Single LSTM layer for faster training\n",
    "        Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Use a more efficient optimizer\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_model_artifacts(model, history, preprocessor, X_test, y_test, y_pred, y_pred_proba, \n",
    "                        selected_features, model_config, base_dir='model_artifacts'):\n",
    "    \"\"\"Save model artifacts with error handling\"\"\"\n",
    "    \n",
    "    # Create base directory and subdirectories\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    subdirs = ['weights', 'logs', 'data', 'config', 'checkpoints', \n",
    "               'evaluation', 'preprocessing', 'visualizations', 'deployment']\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Save model weights with correct extension\n",
    "        model.save_weights(os.path.join(base_dir, 'weights', 'model_weights.weights.h5'))\n",
    "        \n",
    "        # 2. Save complete model with correct extension\n",
    "        model.save(os.path.join(base_dir, 'deployment', 'complete_model.h5'))\n",
    "        \n",
    "        # 3. Save training logs\n",
    "        pd.DataFrame(history.history).to_csv(\n",
    "            os.path.join(base_dir, 'logs', 'training_logs.csv')\n",
    "        )\n",
    "        \n",
    "        # 4. Save preprocessed data samples\n",
    "        np.savez(os.path.join(base_dir, 'data', 'preprocessed_data.npz'),\n",
    "                 X_test=X_test, y_test=y_test)\n",
    "        \n",
    "        # 5. Save model configuration\n",
    "        with open(os.path.join(base_dir, 'config', 'model_config.json'), 'w') as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "        \n",
    "        # 6. Save preprocessing objects\n",
    "        joblib.dump(preprocessor.scaler, \n",
    "                   os.path.join(base_dir, 'preprocessing', 'scaler.pkl'))\n",
    "        joblib.dump(preprocessor.label_encoder, \n",
    "                   os.path.join(base_dir, 'preprocessing', 'label_encoder.pkl'))\n",
    "        \n",
    "        print(f\"\\nAll model artifacts saved successfully in {base_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model artifacts: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def create_distribution_visualizations(X, y, class_names, output_dir='model_artifacts/visualizations'):\n",
    "    \"\"\"\n",
    "    Create t-SNE and PCA visualizations for the final data distribution\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        The feature matrix\n",
    "    y : numpy.ndarray\n",
    "        The target labels\n",
    "    class_names : list\n",
    "        List of class names\n",
    "    output_dir : str\n",
    "        Directory to save the visualizations\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Flatten the sequences if dealing with 3D data (samples, timesteps, features)\n",
    "    if len(X.shape) == 3:\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "    else:\n",
    "        X_flat = X\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_flat)\n",
    "    \n",
    "    # Create PCA visualization\n",
    "    print(\"\\nGenerating PCA visualization...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.title('PCA Visualization of Data Distribution')\n",
    "    plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.3f}')\n",
    "    plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.3f}')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=plt.cm.tab10(i / len(class_names)), \n",
    "                                 label=class_names[i], markersize=10)\n",
    "                      for i in range(len(class_names))]\n",
    "    plt.legend(handles=legend_elements, title='Classes', \n",
    "              bbox_to_anchor=(1.15, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'pca_distribution.png'), \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create t-SNE visualization\n",
    "    print(\"\\nGenerating t-SNE visualization...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, \n",
    "                n_iter=1000, learning_rate='auto')\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.title('t-SNE Visualization of Data Distribution')\n",
    "    plt.xlabel('First t-SNE Component')\n",
    "    plt.ylabel('Second t-SNE Component')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=plt.cm.tab10(i / len(class_names)), \n",
    "                                 label=class_names[i], markersize=10)\n",
    "                      for i in range(len(class_names))]\n",
    "    plt.legend(handles=legend_elements, title='Classes', \n",
    "              bbox_to_anchor=(1.15, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'tsne_distribution.png'), \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create distribution analysis report\n",
    "    create_distribution_report(X_pca, X_tsne, y, class_names, pca, output_dir)\n",
    "\n",
    "def create_distribution_report(X_pca, X_tsne, y, class_names, pca, output_dir):\n",
    "    \"\"\"Create a detailed report of the distribution analysis\"\"\"\n",
    "    report_path = os.path.join(output_dir, 'distribution_analysis_report.txt')\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"Data Distribution Analysis Report\\n\")\n",
    "        f.write(\"================================\\n\\n\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        f.write(\"1. Overall Statistics\\n\")\n",
    "        f.write(\"-----------------\\n\")\n",
    "        f.write(f\"Total samples: {len(y)}\\n\")\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        f.write(\"\\nClass distribution:\\n\")\n",
    "        for cls, count in zip(class_names, class_counts):\n",
    "            f.write(f\"{cls}: {count} samples ({count/len(y)*100:.2f}%)\\n\")\n",
    "        \n",
    "        # PCA analysis\n",
    "        f.write(\"\\n2. PCA Analysis\\n\")\n",
    "        f.write(\"-------------\\n\")\n",
    "        f.write(\"Explained variance ratios:\\n\")\n",
    "        f.write(f\"First component: {pca.explained_variance_ratio_[0]:.4f}\\n\")\n",
    "        f.write(f\"Second component: {pca.explained_variance_ratio_[1]:.4f}\\n\")\n",
    "        f.write(f\"Total variance explained: {sum(pca.explained_variance_ratio_[:2])*100:.2f}%\\n\")\n",
    "        \n",
    "        # Class separation analysis\n",
    "        f.write(\"\\n3. Class Separation Analysis\\n\")\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(\"Mean distances between class centers:\\n\")\n",
    "        \n",
    "        # Calculate class centers in PCA space\n",
    "        centers_pca = np.array([X_pca[y == i].mean(axis=0) for i in range(len(class_names))])\n",
    "        centers_tsne = np.array([X_tsne[y == i].mean(axis=0) for i in range(len(class_names))])\n",
    "        \n",
    "        # Calculate distances between class centers\n",
    "        for i in range(len(class_names)):\n",
    "            for j in range(i + 1, len(class_names)):\n",
    "                dist_pca = np.linalg.norm(centers_pca[i] - centers_pca[j])\n",
    "                dist_tsne = np.linalg.norm(centers_tsne[i] - centers_tsne[j])\n",
    "                f.write(f\"\\n{class_names[i]} vs {class_names[j]}:\\n\")\n",
    "                f.write(f\"  PCA distance: {dist_pca:.4f}\\n\")\n",
    "                f.write(f\"  t-SNE distance: {dist_tsne:.4f}\\n\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = DataPreprocessor()\n",
    "        \n",
    "        # Define the selected features\n",
    "        selected_features = [\n",
    "            # Original High-Importance Features\n",
    "            ' Flow IAT Min',\n",
    "            ' Bwd Avg Packets/Bulk',\n",
    "            'Fwd PSH Flags',\n",
    "            'Init_Win_bytes_forward',\n",
    "            ' PSH Flag Count',\n",
    "            ' Bwd URG Flags',\n",
    "            ' Fwd Packet Length Mean',\n",
    "            ' Fwd IAT Std',\n",
    "            'Fwd Packets/s',\n",
    "            'Bwd Avg Bulk Rate',\n",
    "            ' Destination Port',\n",
    "            'Idle Mean',\n",
    "            ' Packet Length Mean',\n",
    "            # Additional Features for Complete Coverage\n",
    "            ' Flow Duration',\n",
    "            'Active Mean',\n",
    "            ' Active Min',\n",
    "            ' Total Length of Bwd Packets',\n",
    "            ' Bwd Header Length',\n",
    "            ' Subflow Fwd Bytes',\n",
    "            ' Total Fwd Packets'\n",
    "        ]\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        print(\"\\nLoading dataset...\")\n",
    "        data = load_dataset()\n",
    "        \n",
    "        print(\"\\nCleaning data...\")\n",
    "        data = preprocessor.load_and_clean_data(data)\n",
    "        \n",
    "        # Print available columns\n",
    "        print(\"\\nAvailable columns in dataset:\")\n",
    "        print(data.columns.tolist())\n",
    "        \n",
    "        # Verify all selected features are in the dataset\n",
    "        missing_features = [f for f in selected_features if f not in data.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features in dataset: {missing_features}\")\n",
    "        \n",
    "        print(\"\\nUsing selected features:\")\n",
    "        print(selected_features)\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        print(\"\\nPreparing features and labels...\")\n",
    "        X = data[selected_features]\n",
    "        y = data[' Label']\n",
    "        \n",
    "        # Free memory\n",
    "        del data\n",
    "        \n",
    "        # Scale features\n",
    "        print(\"\\nScaling features...\")\n",
    "        X_scaled = preprocessor.scaler.fit_transform(X)\n",
    "        y_encoded = preprocessor.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Free more memory\n",
    "        del X\n",
    "        \n",
    "        # Balance dataset\n",
    "        print(\"\\nBalancing dataset...\")\n",
    "        X_balanced, y_balanced = preprocessor.balance_dataset(\n",
    "            X_scaled, y_encoded,\n",
    "            max_samples_per_class=10000\n",
    "        )\n",
    "        \n",
    "        # Create sequences\n",
    "        print(\"\\nCreating sequences...\")\n",
    "        X_seq, y = preprocessor.create_sequences(X_balanced, y_balanced)\n",
    "        \n",
    "        # Split data\n",
    "        print(\"\\nSplitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seq, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Validate data shapes before training\n",
    "        print(\"\\nValidating data shapes:\")\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "        print(f\"y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        if len(X_train.shape) != 3:\n",
    "            raise ValueError(f\"Expected X_train to be 3D, got shape {X_train.shape}\")\n",
    "        \n",
    "        # Define input shape and number of classes\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # Save data shapes for model recovery\n",
    "        np.savez('data_shapes.npz',\n",
    "                 input_shape=input_shape,\n",
    "                 n_classes=n_classes)\n",
    "        \n",
    "        # Create and train improved model\n",
    "        print(\"\\nCreating and training improved model...\")\n",
    "        \n",
    "        # Create model configuration\n",
    "        model_config = {\n",
    "            'architecture': 'CNN-LSTM',\n",
    "            'input_shape': input_shape,\n",
    "            'n_classes': n_classes,\n",
    "            'hyperparameters': {\n",
    "                'initial_learning_rate': 0.001,\n",
    "                'batch_size': 32,\n",
    "                'epochs': 50\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create model\n",
    "        model = create_improved_model(input_shape, n_classes)\n",
    "        \n",
    "        # Print model summary\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Save initial model weights\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        model.save_weights('checkpoints/initial_weights.weights.h5')\n",
    "        \n",
    "        # Train model with try-except\n",
    "        try:\n",
    "            trainer = ModelTrainer(model)\n",
    "            history = trainer.train_with_improvements(X_train, y_train)\n",
    "            \n",
    "            # Save training history immediately\n",
    "            with open('training_history.pkl', 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "                \n",
    "        except Exception as train_error:\n",
    "            logging.error(f\"Error during training: {str(train_error)}\")\n",
    "            # Try to load last checkpoint\n",
    "            latest_checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "            if latest_checkpoint:\n",
    "                print(\"\\nAttempting to load last checkpoint...\")\n",
    "                model.load_weights(latest_checkpoint)\n",
    "            raise\n",
    "            \n",
    "        # Generate predictions with error handling\n",
    "        try:\n",
    "            y_pred = model.predict(X_test, batch_size=32)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "            # Save predictions immediately\n",
    "            np.savez('predictions.npz',\n",
    "                    y_pred=y_pred,\n",
    "                    y_pred_classes=y_pred_classes)\n",
    "                    \n",
    "        except Exception as pred_error:\n",
    "            logging.error(f\"Error during prediction: {str(pred_error)}\")\n",
    "            raise\n",
    "        \n",
    "        # Save all model artifacts\n",
    "        save_model_artifacts(\n",
    "            model=model,\n",
    "            history=history,\n",
    "            preprocessor=preprocessor,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            y_pred=y_pred_classes,\n",
    "            y_pred_proba=y_pred,\n",
    "            selected_features=selected_features,\n",
    "            model_config=model_config\n",
    "        )\n",
    "        \n",
    "        # Continue with evaluation\n",
    "        trainer.evaluate(X_test, y_test, preprocessor.label_encoder)\n",
    "        #Distribution Visualizations\n",
    "        print(\"\\nGenerating distribution visualizations...\")\n",
    "        create_distribution_visualizations(\n",
    "            X_test.reshape(X_test.shape[0], -1),  # Flatten sequences\n",
    "            y_test,\n",
    "            preprocessor.label_encoder.classes_,\n",
    "            output_dir='model_artifacts/visualizations'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nVisualization artifacts have been saved to 'model_artifacts/visualizations'\")\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def objective(trial, X_train, y_train, input_shape, n_classes):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128)\n",
    "    \n",
    "    # Create model with trial parameters\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, padding='same', activation='relu',\n",
    "               kernel_regularizer=l2(l2_reg), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Bidirectional(LSTM(lstm_units_1, return_sequences=True, \n",
    "                          kernel_regularizer=l2(l2_reg))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Bidirectional(LSTM(lstm_units_2, kernel_regularizer=l2(l2_reg))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return history.history['val_loss'][-1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
